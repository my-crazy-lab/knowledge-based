# Storage and Retrieval

## Storing values within the index

- Khóa trong chỉ mục là thứ mà truy vấn tìm kiếm, nhưng giá trị có thể là một trong hai dạng: 
    - hoặc là toàn bộ dòng dữ liệu (tài liệu, đỉnh)
    - hoặc là tham chiếu đến dòng được lưu trữ ở nơi khác
    - Trường hợp thứ hai, nơi lưu trữ các dòng được gọi là "heap file": lưu dữ liệu không theo thứ tự cụ thể nào
    - Cách tiếp cận heap file khá phổ biến vì nó tránh việc trùng lặp dữ liệu khi có nhiều chỉ mục phụ: mỗi chỉ mục chỉ cần tham chiếu đến vị trí trong heap, còn dữ liệu thực được lưu một chỗ duy nhất.
- Khi cập nhật giá trị mà không thay đổi khóa, phương pháp heap file khá hiệu quả
    - Chỉ cần ghi đè bản ghi tại chỗ nếu giá trị mới không dài hơn giá trị cũ(size of mem)
    - Nhưng nếu giá trị mới dài hơn, có thể cần chuyển bản ghi sang một vị trí mới trong heap có đủ chỗ.
        - hoặc là tất cả chỉ mục cần được cập nhật để trỏ đến vị trí mới
        - hoặc là để lại một con trỏ chuyển tiếp ở vị trí cũ.
    - bước truy cập gián tiếp từ chỉ mục đến heap gây ảnh hưởng hiệu suất đọc
        - ta có thể lưu trực tiếp dòng dữ liệu trong chỉ mục, gọi là **clustered index**
- **covering index** hoặc **index with included columns**, cho phép lưu một vài cột trong chỉ mục
- việc trùng lặp dữ liệu (clustered và covering indexes) tuy giúp tăng tốc độ đọc, nhưng lại yêu cầu thêm không gian lưu trữ và làm phức tạp quá trình ghi dữ liệu.

## Multi-column indexes

- **concatenated index**: kết hợp nhiều cột thành một khóa duy nhất bằng cách nối chúng theo một thứ tự xác định
    - Giống như danh bạ giấy cũ, sắp xếp theo (họ, tên) đến số điện thoại. Bạn có thể tìm theo họ, hoặc họ + tên, nhưng không thể chỉ tìm theo tên
- **multi-dimensional indexes**: giúp xử lý các truy vấn nhiều chiều tốt hơn, đặc biệt là với dữ liệu không gian
    - Ví dụ, nếu bạn muốn tìm các nhà hàng trong khu vực hiển thị trên bản đồ, bạn cần truy vấn phạm vi hai chiều (latitude, longitude). Chỉ mục B-tree hoặc LSM-tree truyền thống không hỗ trợ tốt kiểu truy vấn này.
    - **Space-Filling Curve**
        - Space-filling curve (đường cong lấp đầy không gian) là một cách ánh xạ dữ liệu đa chiều thành một chiều, trong khi vẫn giữ gần đúng tính lân cận giữa các điểm.
            - Ex: Dùng Z-order curve (Morton code) hoặc Hilbert curve để chuyển tọa độ (x, y) thành một số nguyên duy nhất.
        - Sau đó, bạn có thể dùng một B-tree truyền thống để lập chỉ mục vì dữ liệu giờ đã "đơn chiều".
        - Ưu điểm:
            - Không cần thay đổi cấu trúc chỉ mục (vẫn dùng được B-tree).
            - Giữ được hiệu suất tương đối tốt cho nhiều truy vấn.
        - Hạn chế:
            - Không chính xác tuyệt đối về khoảng cách/lân cận.
            - Cần xử lý phức tạp hơn để chuyển đổi tọa độ.
    - **R-tree**
        - được thiết kế đặc biệt cho dữ liệu đa chiều như (latitude, longitude).
        - là một cấu trúc cây dùng để lưu trữ và truy vấn dữ liệu hình học nhiều chiều
        - R-tree là cây nhiều nhánh, giống B-tree, nhưng mỗi nút (node) chứa:
            - Một danh sách các bounding box.
                - Bounding box (hộp giới hạn) là hình chữ nhật nhỏ nhất bao quanh một đối tượng.
            - Mỗi bounding box bao quanh các bounding box con bên trong (→ lồng nhau).

## fuzzy indexes

- Các chỉ mục trên đến nay đều yêu cầu giá trị chính xác, nhưng trong tìm kiếm toàn văn, bạn cần hỗ trợ tìm từ đồng nghĩa, dạng biến đổi ngữ pháp, hoặc sai chính tả.
    - **Lucene** là một ví dụ điển hình
    - Nó dùng cấu trúc giống SSTable để lưu từ điển thuật ngữ, nhưng có một chỉ mục trong bộ nhớ dạng **finite state automaton**(tự động hữu hạn) như cây **trie**, cho phép tìm từ trong một khoảng cách chỉnh sửa nhất định(edit distance)

## in-memory databases

- Các cấu trúc dữ liệu kể trên được thiết kế để vượt qua hạn chế của ổ đĩa. 
    - Nhưng khi RAM trở nên rẻ hơn và nhiều dữ liệu không lớn đến mức vượt quá bộ nhớ, thì việc lưu trữ toàn bộ trong bộ nhớ trở nên khả thi.
- Một số hệ thống như Memcached chỉ là bộ đệm, chấp nhận mất dữ liệu nếu tắt máy. Nhưng nhiều hệ thống khác như VoltDB, Redis, MemSQL hay Oracle TimesTen hỗ trợ tính bền vững (durability) bằng cách ghi log hoặc snapshot ra đĩa, hoặc sao lưu giữa các máy
- **anti-caching**: cơ sở dữ liệu vẫn hoạt động chủ yếu trên RAM, nhưng có thể hoán đổi các bản ghi ít được dùng ra đĩa

## Column-Oriented Storage

![This is a screenshot](Figure310.png)  

- Với lưu trữ theo hàng, engine vẫn cần tải tất cả các dòng (mỗi dòng có hơn 100 thuộc tính) từ đĩa vào bộ nhớ, phân tích cú pháp, và lọc ra những dòng không phù hợp — điều này có thể tốn rất nhiều thời gian.
- Thay vì lưu tất cả giá trị của một dòng cùng nhau, ta lưu tất cả giá trị của mỗi cột cùng nhau. Nếu mỗi cột được lưu trong một tệp riêng, truy vấn chỉ cần đọc và phân tích các cột thực sự được sử dụng trong truy vấn, từ đó tiết kiệm được rất nhiều công sứ

### Column Compression

![This is a screenshot](Figure311.png)  

- giảm yêu cầu về băng thông đĩa bằng cách nén dữ liệu
- lưu trữ theo cột thường rất phù hợp với việc nén
- **bitmap encoding**
    - Thông thường, số lượng giá trị khác nhau (distinct) trong một cột là nhỏ so với tổng số dòng
    - lấy một cột với n giá trị khác nhau, và chuyển nó thành n bitmap riêng biệt
        - mỗi bitmap tương ứng với một giá trị, và mỗi bit đại diện cho một dòn
    - Bit có giá trị 1 nếu dòng đó có giá trị tương ứng, và 0 nếu không.
    - Nếu n rất nhỏ (ví dụ, cột quốc gia có khoảng 200 giá trị khác nhau), các bitmap đó có thể lưu một bit cho mỗi dòng.
    - Nhưng nếu n lớn hơn, phần lớn các bitmap sẽ chứa nhiều số 0 — ta gọi chúng là **sparse bitmaps** (bitmap thưa)
        - Trong trường hợp đó, bitmap có thể được mã hóa theo kiểu run-length encoding (RLE) để giảm kích thước lưu trữ.

### vectorized processing

- Đối với những truy vấn kho dữ liệu cần quét qua hàng triệu dòng, **big bottleneck** là băng thông từ đĩa lên bộ nhớ
- Nhưng đó không phải là trở ngại duy nhất
    - Việc tối ưu băng thông từ bộ nhớ chính lên cache của CPU
    - Tránh các dự đoán nhánh sai (branch mispredictions) và các “bong bóng” (bubble) trong pipeline xử lý lệnh của CPU
    - Tận dụng các lệnh SIMD (single-instruction-multi-data) trên các CPU hiện đại [59, 60]
- Ngoài việc giảm khối lượng dữ liệu cần tải từ đĩa, bố cục lưu trữ theo cột cũng giúp sử dụng chu kỳ CPU hiệu quả hơn
    - Ví dụ, engine truy vấn có thể lấy một khối dữ liệu cột đã nén sao cho vừa vặn trong L1 cache của CPU, sau đó lặp qua nó trong một vòng lặp đơn giản (tight loop — không có gọi hàm hay điều kiện rẽ nhánh). CPU có thể xử lý vòng lặp này nhanh hơn nhiều so với đoạn mã có nhiều hàm gọi hoặc điều kiện phân nhánh.
- Việc nén cột giúp nhiều dòng hơn có thể nằm trong cùng một cache L1, và các phép toán như **bitwise AND/OR** được thiết kế để hoạt động trực tiếp trên các khối dữ liệu nén. Kỹ thuật này gọi là **vectorized processing**

### Sort Order in Column Storage

- Cách đơn giản nhất là lưu trữ các dòng theo thứ tự chèn vào, vì khi đó thêm một dòng mới chỉ đơn giản là thêm vào cuối mỗi tệp cột
- Tuy nhiên, chúng ta có thể áp dụng một thứ tự sắp xếp nhất định, giống như cách đã làm với SSTables, và sử dụng thứ tự đó như một cơ chế lập chỉ mục.
- Lưu ý rằng không thể sắp xếp từng cột một cách độc lập, vì khi đó chúng ta sẽ không thể biết các giá trị nào thuộc cùng một dòng. Chúng ta chỉ có thể tái tạo lại một dòng vì biết rằng mục thứ k trong một cột tương ứng với mục thứ k trong tất cả các cột khác.
- dữ liệu cần phải được sắp xếp toàn bộ theo từng dòng, mặc dù lưu trữ là theo cột
- Một lợi ích khác của việc sắp xếp là giúp nén dữ liệu hiệu quả hơn. Nếu cột dùng để sắp xếp không có nhiều giá trị khác nhau, thì sau khi sắp xếp, cột đó sẽ có nhiều chuỗi giá trị giống nhau liên tiếp.
    - Hiệu quả nén này sẽ cao nhất với khóa sắp xếp đầu tiên. Khóa thứ hai và thứ ba sẽ bị “lẫn lộn” nhiều hơn, do đó các chuỗi giá trị giống nhau sẽ ngắn hơn và khó nén hơn
    - Càng về sau trong thứ tự sắp xếp, cột càng trở nên gần như ngẫu nhiên, nên khả năng nén thấp hơn. Tuy nhiên, việc có vài cột đầu tiên được sắp xếp vẫn là một lợi ích đáng kể.
- **C-Stor**
    - Dữ liệu cần được sao lưu trên nhiều máy để đảm bảo an toàn (phòng khi một máy bị hỏng).
    - Do đó, ta có thể lưu phiên bản sao của dữ liệu theo các thứ tự sắp xếp khác nhau, và khi xử lý truy vấn, chọn ra phiên bản phù hợp nhất với kiểu truy vấn.

### Writing to Column-Oriented Storage

- điểm yếu là việc ghi dữ liệu trở nên khó khăn hơn.
- Không thể áp dụng cách cập nhật trực tiếp (update-in-place) như B-trees cho các cột đã được nén. Nếu bạn muốn chèn một dòng vào giữa bảng đã sắp xếp, có thể sẽ phải viết lại toàn bộ các tệp cột.
    - Vì dòng được xác định bằng vị trí trong cột, nên khi chèn, bạn phải cập nhật tất cả các cột cùng một lúc.
- giải pháp: LSM-trees
    - Tất cả các thao tác ghi ban đầu được lưu vào bộ nhớ (in-memory store), nơi chúng được thêm vào cấu trúc đã sắp xếp và chuẩn bị để ghi ra đĩa. Không quan trọng bộ nhớ này lưu theo hàng hay theo cột.
    - Khi số lượng ghi đủ lớn, dữ liệu sẽ được hợp nhất với các tệp cột trên đĩa và ghi thành các tệp mới một cách hàng loạt
    - Các truy vấn cần phải kiểm tra cả dữ liệu trên đĩa lẫn các bản ghi mới trong bộ nhớ, rồi kết hợp hai phần
        - Khi có dữ liệu mới (chèn, cập nhật, hoặc xóa), hệ thống không ghi ngay vào các tệp cột trên đĩa.
        - Thay vào đó, nó ghi tạm vào vùng nhớ gọi là "write buffer" (bộ nhớ ghi), hoặc in-memory delta store.
        - Dữ liệu trong bộ nhớ được tổ chức theo dạng cấu trúc có thể truy vấn (thường là sorted maps, tree hoặc mini-column store).
        - Các thao tác ghi sẽ được ghi lại vào WAL (Write-Ahead Log) để đảm bảo an toàn dữ liệu nếu hệ thống bị crash.

### Aggregation: Data Cubes and Materialized Views

![This is a screenshot](Figure312.png)  

- các truy vấn trong kho dữ liệu thường bao gồm các hàm tổng hợp như COUNT, SUM, AVG, MIN, hoặc MAX trong SQL. Nếu nhiều truy vấn khác nhau đều sử dụng cùng một phép tổng hợp, thì việc phải xử lý lại dữ liệu gốc mỗi lần có thể gây lãng phí tài nguyên
    - Vậy tại sao không lưu sẵn (cache) các giá trị tổng hợp được sử dụng thường xuyên?
- *materialized view*
    -  thường được định nghĩa giống như **virtual view**: là một đối tượng giống như bảng, mà nội dung là kết quả của một truy vấn
    - Điểm khác biệt: **materialized view** là một bản sao thực sự của kết quả truy vấn, được ghi xuống đĩa, trong khi **virtual view** chỉ đơn giản là một cách viết tắt của truy vấn.
    - Khi dữ liệu gốc thay đổi, materialized view cũng cần được cập nhật, bởi vì nó là một bản sao đã được "phi chuẩn hóa" của dữ liệu -> các thao tác ghi trở nên tốn kém hơn
- trường hợp phổ biến của **materialized view** là khối dữ liệu **(data cube)** hoặc **OLAP cube**
    - đa số các kho dữ liệu hiện nay vẫn cố gắng giữ càng nhiều dữ liệu gốc càng tốt, và chỉ sử dụng các tổng hợp như data cube như một cách tăng hiệu năng cho một số truy vấn cụ thể.

## Map reduce

- MapReduce là một mô hình lập trình để xử lý một lượng lớn dữ liệu hàng loạt trên nhiều máy tính, được Google phổ biến

## Graph db

- khi các kết nối trong dữ liệu của bạn trở nên phức tạp hơn, thì việc mô hình hóa dữ liệu dưới dạng đồ thị trở nên tự nhiên hơn.

## So sánh cơ sở dữ liệu đồ thị với mô hình mạng

| Khía cạnh               | CODASYL (CSDL mạng)                 | CSDL Đồ thị                                  |
|-------------------------|-------------------------------------|----------------------------------------------|
| **Sơ đồ cố định**       | Có                                 | Không bắt buộc                               |
| **Truy cập bản ghi**    | Theo đường dẫn                     | Trực tiếp qua ID hoặc chỉ mục (index)        |
| **Thứ tự bản ghi con**  | Có thứ tự bắt buộc                 | Không có thứ tự                               |
| **Truy vấn**            | Mệnh lệnh, chi tiết                 | Hỗ trợ khai báo (Cypher, SPARQL, v.v.)        |
| **Tính linh hoạt**      | Thấp                               | Cao                                           |
| **Phù hợp cho**         | Hệ thống cố định, quan hệ sâu       | Dữ liệu linh hoạt, quan hệ phức tạp          |

# Encoding and Evolution

## Formats for Encoding Data

- Chương trình thường làm việc với dữ liệu dưới (ít nhất) hai hình thức khác nhau:
    -  Trong bộ nhớ, dữ liệu được giữ trong các đối tượng (objects), cấu trúc (structs), danh sách (lists), mảng (arrays), bảng băm (hash tables), cây (trees), v.v... Những cấu trúc dữ liệu này được tối ưu để truy xuất và thao tác hiệu quả bởi CPU (thường sử dụng con trỏ).
    - Khi bạn muốn ghi dữ liệu ra tệp hoặc gửi qua mạng, bạn phải mã hóa nó dưới dạng một chuỗi byte độc lập (ví dụ, một tài liệu JSON). Vì con trỏ không có ý nghĩa gì đối với các tiến trình khác, nên cách biểu diễn dưới dạng chuỗi byte này sẽ trông rất khác so với cấu trúc dữ liệu trong bộ nhớ.
        - Trừ một số trường hợp đặc biệt, chẳng hạn như một số tệp được ánh xạ bộ nhớ (memory-mapped files) hoặc khi thao tác trực tiếp trên dữ liệu nén
- Việc chuyển đổi từ biểu diễn trong bộ nhớ sang chuỗi byte được gọi là **encoding**, ngược lại la **decoding**
- vấn đề
    - Mã hóa thường bị ràng buộc với một ngôn ngữ lập trình cụ thể, và rất khó để đọc dữ liệu đó trong một ngôn ngữ khác. 
        - Nếu bạn lưu trữ hoặc truyền dữ liệu dưới dạng như vậy, bạn đang “cam kết” gắn bó lâu dài với ngôn ngữ hiện tại, và loại trừ khả năng tích hợp hệ thống với tổ chức khác
    - Để khôi phục dữ liệu về đúng kiểu đối tượng ban đầu, quá trình giải mã cần khả năng khởi tạo (instantiate) các lớp bất kỳ. Đây là một lỗ hổng bảo mật phổ biến
        - nếu kẻ tấn công có thể khiến ứng dụng của bạn giải mã một chuỗi byte bất kỳ, họ có thể tạo ra các đối tượng tùy ý, điều này thường cho phép họ thực hiện các hành vi nguy hiểm như thực thi mã từ xa
    - Việc quản lý phiên bản dữ liệu thường không được chú trọng
        - vì chúng được thiết kế để mã hóa dữ liệu nhanh chóng và dễ dàng, nên chúng thường bỏ qua các vấn đề phức tạp liên quan đến khả năng tương thích xuôi và ngược
    - Hiệu suất (thời gian CPU để mã hóa/giải mã và kích thước dữ liệu sau khi mã hóa) cũng thường bị bỏ qua
- XML thường bị chê là quá dài dòng và phức tạp một cách không cần thiết
- JSON được ưa chuộng chủ yếu là nhờ vào việc nó được hỗ trợ sẵn trong trình duyệt web
- JSON, XML và CSV đều là định dạng văn bản, ngoài những vấn đề cú pháp bề mặt, chúng còn tồn tại một số vấn đề tinh vi hơn:
    - Có nhiều điểm mơ hồ trong cách mã hóa số
        - XML và CSV không thể phân biệt một số với một chuỗi ký tự chỉ toàn chữ số
        - JSON có phân biệt giữa chuỗi và số, nhưng không phân biệt giữa số nguyên và số thực, cũng không chỉ định độ chính xác.
    - JSON và XML hỗ trợ tốt cho chuỗi ký tự Unicode (tức là văn bản có thể đọc được), nhưng không hỗ trợ chuỗi nhị phân (tức là chuỗi byte không có mã hóa ký tự)
        - Để khắc phục, người ta thường mã hóa nhị phân thành văn bản bằng Base64, sau đó dùng schema để xác định rằng giá trị đó cần được giải mã từ Base64
        - Tuy làm được, nhưng cách này khá "lắt léo" và làm tăng kích thước dữ liệu thêm 33%.
    - Cả XML và JSON đều có hỗ trợ schema tùy chọn 
        - nhưng cũng vì thế mà phức tạp để học và triển khai
    - CSV thì hoàn toàn không có schema

### Binary encoding

- định dạng JSON nhị phân
    - MessagePack
        - ![This is a screenshot](Figure41_2.png)  
        - Kết quả là bản mã hóa nhị phân dài 66 byte, chỉ ít hơn một chút so với 81 byte của JSON gốc (sau khi loại bỏ khoảng trắng). Tất cả định dạng nhị phân cho JSON đều có hiệu quả tương tự. 
        - Do đó, không rõ liệu việc tiết kiệm vài byte và có thể tăng tốc độ phân tích có thực sự xứng đáng với việc đánh đổi khả năng đọc được bằng mắt thường.
    - BSON
    - BJSON
    - UBJSON
    - BISON
    - Smile
- XML thì có WBXML, Fast Infoset, v.v.

## Thrift and Protocol Buffers

![This is a screenshot](Figure42_2.png)  

- Apache Thrift và Protocol Buffers (protobuf) là thư viện mã hóa nhị phân
- đều yêu cầu schema cho bất kỳ dữ liệu nào được mã hóa. 
- đều đi kèm với công cụ sinh mã (code generation tool)

### Lưu ý quan trọng về "required" và "optional"

- Trong các schema đã trình bày, bạn thấy rằng mỗi trường được đánh dấu là required (bắt buộc) hoặc optional (tùy chọn).
- Tuy nhiên, điều này không ảnh hưởng đến cách dữ liệu được mã hóa – trong dữ liệu nhị phân không có dấu hiệu nào cho biết trường nào là bắt buộc hay không.
- Khác biệt duy nhất là ở runtime (lúc chạy chương trình), các trường required sẽ được kiểm tra và nếu bạn quên set giá trị, thì chương trình sẽ báo lỗi

### Field tags and schema evolution

`Tính tương thích là một mối quan hệ giữa một tiến trình mã hóa dữ liệu và một tiến trình khác giải mã dữ liệu đó.`

- schema luôn cần thay đổi theo thời gian, và quá trình này được gọi là tiến hóa schema (**schema evolution**)
- số thẻ (field tag) là yếu tố cốt lõi để hiểu dữ liệu đã mã hóa.
- có thể đổi tên trường trong schema (vì dữ liệu mã hóa không bao giờ dùng đến tên trường),
nhưng không thể đổi số thẻ, vì làm vậy sẽ khiến tất cả dữ liệu cũ trở nên không hợp lệ.
- có thể thêm trường mới vào schema, miễn là bạn gán cho nó một số thẻ chưa từng dùng.
- đảm bảo **forward compatibility**
    - Nếu một đoạn mã cũ (không biết đến các trường mới) đọc dữ liệu được ghi bởi mã mới, nó sẽ thấy các trường có số thẻ không nhận ra – lúc đó, nó sẽ bỏ qua các trường đó. Kiểu dữ liệu đi kèm sẽ giúp parser biết phải bỏ qua bao nhiêu byte.
- đảm bảo **backward compatibility**
    - Chừng nào mỗi trường vẫn giữ nguyên số thẻ, thì mã mới vẫn có thể đọc dữ liệu từ mã cũ, vì các số thẻ vẫn mang ý nghĩa giống nhau.
    - Nếu thêm một trường mới, thì không được đánh dấu là required. Vì nếu bạn làm vậy, khi mã mới đọc dữ liệu cũ (chưa có trường mới này), nó sẽ báo lỗi do thiếu trường bắt buộc.
    - mọi trường thêm sau khi schema đã triển khai nên được đánh dấu là optional hoặc phải có giá trị mặc định (default value).
- chỉ có thể xóa những trường optional (trường required thì không bao giờ được xóa).
- không bao giờ được tái sử dụng số thẻ cũ – vì có thể vẫn còn dữ liệu đã ghi trước đây với số thẻ đó, và mã mới cần biết để bỏ qua đúng cách.

### Datatypes and schema evolution

- thay đổi kiểu dữ liệu: có nguy cơ dữ liệu sẽ bị mất độ chính xác hoặc bị cắt cụt
- Ví dụ:
    - Bạn đổi một số nguyên 32-bit (int32) thành số nguyên 64-bit (int64)
    - Mã mới có thể đọc dữ liệu từ mã cũ, vì có thể điền thêm các bit còn thiếu bằng 0
    - Nhưng nếu mã cũ đọc dữ liệu từ mã mới, nó sẽ dùng biến int32 để giữ giá trị int64 → Nếu giá trị int64 quá lớn, nó sẽ bị cắt cụt (truncated).
- Protocol Buffers không có kiểu dữ liệu danh sách (list) hay mảng (array).
    - Thay vào đó, nó sử dụng từ khóa **repeated** cho các trường – như một lựa chọn thứ ba bên cạnh required và optional.
    - Ưu điểm
        - có thể chuyển một trường đơn (optional) thành một danh sách (**repeated**) mà không làm hỏng dữ liệu cũ
- Thrift có kiểu dữ liệu list riêng biệt, hỗ trợ danh sách lồng nhau (**nested lists**)

## Avro

- hỗ trợ hai ngôn ngữ định nghĩa schema:
    - Một ngôn ngữ có thể chỉnh sửa dễ dàng bằng tay (**Avro IDL**)
    - Một ngôn ngữ dựa trên JSON, dễ đọc và xử lý bằng máy
- Schema của Avro không có số thẻ (tag numbers) như trong Protocol Buffers hay Thrift.
    - Trong chuỗi byte được mã hóa, bạn sẽ thấy không có thông tin nào để xác định trường hay kiểu dữ liệu.
    - Mỗi giá trị chỉ đơn giản được nối lại với nhau, theo thứ tự đã định trong schema.
    - Điều này có nghĩa
        - Việc giải mã dữ liệu nhị phân chỉ có thể thực hiện đúng nếu trình đọc sử dụng chính xác cùng schema với trình ghi.
        - Bất kỳ sai lệch nào giữa schema của người đọc và người ghi đều có thể khiến dữ liệu giải mã sai.

### Schema evolution rules

- Khi một ứng dụng muốn mã hóa dữ liệu (ghi vào file, gửi qua mạng,...), nó sử dụng schema mà nó biết tại thời điểm đó – schema đó được gọi là **writer's schema**.
- Khi một ứng dụng muốn đọc dữ liệu, nó sử dụng một schema để giải mã – được gọi là **reader's schema**.
- Writer's schema và reader's schema không cần phải giống nhau – chúng chỉ cần tương thích (compatible).
- **forward compatibility**: có thể dùng schema mới để ghi, và schema cũ để đọc
- **backward compatibility**: có thể dùng schema mới để đọc, và schema cũ để ghi
- Để duy trì sự tương thích:
    - Bạn chỉ có thể thêm hoặc xóa trường nếu trường đó có giá trị mặc định
- Lưu ý kỹ thuật
    - Giá trị mặc định phải có kiểu giống với nhánh đầu tiên trong kiểu **union**. Đây là hạn chế cụ thể của Avro
    - Trong một số ngôn ngữ lập trình, **null** có thể dùng làm mặc định cho mọi biến. Nhưng trong Avro thì không phải như vậy
        - Muốn một trường có thể nhận giá trị **null** → bạn phải dùng **union**
        - Chỉ khi null là một phần của union, bạn mới có thể dùng nó làm default value 
        - Điều này có vẻ rườm rà hơn, nhưng lại giúp giảm lỗi bằng cách rõ ràng hóa việc gì có thể null, cái gì không.

#### writer’s schema

- Làm sao trình đọc biết được schema mà trình ghi đã dùng để mã hóa dữ liệu?
    - Câu trả lời phụ thuộc vào ngữ cảnh sử dụng Avro
        - File lớn chứa nhiều bản ghi
            - Ghi vào file lớn chứa hàng triệu bản ghi, tất cả đều dùng cùng một schema.
        - Cơ sở dữ liệu (database) với các bản ghi được ghi riêng lẻ
            - Các bản ghi có thể được ghi vào ở những thời điểm khác nhau, với schema khác nhau
            - không thể giả định rằng tất cả bản ghi đều có chung schema
            - Giải pháp 
                - Gắn thêm một số phiên bản (version number) ở đầu mỗi bản ghi
                - Lưu danh sách các schema tương ứng với từng version trong CSDL
        -  Gửi bản ghi qua kết nối mạng
            - Chúng có thể thương lượng phiên bản schema khi kết nối được thiết lập
            - Sau đó, chúng sử dụng schema đó trong suốt quá trình truyền dữ liệu
            - `Giao thức RPC của Avro hoạt động theo cách này`

#### dynamically generated schemas

- Một ưu điểm lớn của Avro so với Protocol Buffers và Thrift là:
    - Schema của Avro không có tag number
    - Avro: schema được tạo ra một cách động.
    - Thrift hay Protocol Buffers không được thiết kế để hỗ trợ schema tạo động
- Thrift và Protocol Buffers phụ thuộc vào code generation
    - rất tiện trong ngôn ngữ kiểu tĩnh
        - Tối ưu cấu trúc dữ liệu trong bộ nhớ
        - IDE hỗ trợ autocomplete, kiểm tra kiểu, v.v.
    - but with ngôn ngữ kiểu động như JavaScript, Ruby, Python
        - Không có bước kiểm tra kiểu tại thời gian biên dịch → code generation trở nên không cần thiết
        - Thậm chí còn bị xem là rườm rà, vì các ngôn ngữ này vốn tránh bước biên dịch
- Avro rất phù hợp với ngôn ngữ kiểu động

## Dataflow Through Databases

- tính tương thích lùi rõ ràng là cần thiết; nếu không, "phiên bản tương lai" của bạn sẽ không thể giải mã được những gì  đã viết trước đó

## Dataflow Through Services: REST and RPC

- REST không phải là một giao thức, mà là một triết lý thiết kế xây dựng dựa trên các nguyên lý của HTTP. Nó nhấn mạnh các định dạng dữ liệu đơn giản, sử dụng URL để xác định tài nguyên và sử dụng các tính năng HTTP cho điều khiển bộ nhớ đệm, xác thực và thương lượng loại nội dung
- Mô hình RPC cố gắng làm cho một yêu cầu đến dịch vụ mạng từ xa trông giống như một cuộc gọi hàm hoặc phương thức trong ngôn ngữ lập trình của bạn, trong cùng một tiến trình
- Mặc dù RPC có vẻ thuận tiện lúc đầu, nhưng cách tiếp cận này về cơ bản có vấn đề. Một yêu cầu mạng khác biệt rất nhiều so với một cuộc gọi hàm nội bộ:
    - không có lý do gì để cố gắng làm cho một dịch vụ từ xa trông giống như một đối tượng nội bộ trong ngôn ngữ lập trình của bạn, vì nó là một thứ hoàn toàn khác biệt
- Mã hóa dữ liệu và sự tiến hóa cho RPC
    - chỉ cần khả năng tương thích ngược đối với yêu cầu, và khả năng tương thích tiến tới đối với phản hồi.
    - Tương thích dịch vụ trở nên khó khăn hơn do thực tế là RPC thường được sử dụng để giao tiếp qua ranh giới tổ chức, vì vậy nhà cung cấp dịch vụ thường không kiểm soát các khách hàng của mình và không thể ép buộc họ nâng cấp

## Message-Passing Dataflow

- Việc sử dụng message broker có một số lợi thế so với RPC trực tiếp:
    - Nó có thể hoạt động như một bộ đệm nếu bên nhận không có sẵn hoặc quá tải, từ đó cải thiện độ tin cậy của hệ thống.
    - Nó có thể tự động gửi lại tin nhắn đến một quá trình bị hỏng, giúp ngăn ngừa việc mất tin nhắn.
    - Nó tránh việc người gửi cần phải biết địa chỉ IP và số cổng của người nhận (đặc biệt hữu ích trong môi trường đám mây, nơi các máy ảo thường thay đổi liên tục).
    - Nó cho phép một tin nhắn được gửi đến nhiều người nhận.
    - Nó phân tách lôgic giữa người gửi và người nhận (người gửi chỉ việc gửi tin nhắn mà không quan tâm đến ai sẽ tiêu thụ chúng).

### Distributed Actor Frameworks

- Mô hình actor là một mô hình lập trình cho việc đồng thời hóa trong một quá trình đơn. Thay vì xử lý trực tiếp các luồng (và các vấn đề liên quan đến điều kiện đua, khóa, và deadlock), logic được đóng gói trong các actor.
- Mỗi actor đại diện cho một khách hàng hoặc thực thể, có thể có một số trạng thái cục bộ (không chia sẻ với bất kỳ actor nào khác), và nó giao tiếp với các actor khác thông qua việc gửi và nhận các tin nhắn bất đồng bộ
- Vì mỗi actor chỉ xử lý một tin nhắn tại một thời điểm, nó không cần lo lắng về các luồng, và mỗi actor có thể được lập lịch độc lập bởi khung làm việc.
- **location transparency** hoạt động tốt hơn trong mô hình actor so với RPC
